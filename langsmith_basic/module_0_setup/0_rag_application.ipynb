{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUWIE78_2wj3"
   },
   "source": [
    "# RAG Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVYmp6Po2wj7"
   },
   "source": [
    "![Simple RAG](../images/simple_rag.png)\n",
    "\n",
    "In this notebook, we're going to set up a simple RAG application that we'll be using as we learn more about LangSmith.\n",
    "\n",
    "RAG (Retrieval Augmented Generation) is a popular technique for providing LLMs with relevant documents that will enable them to better answer questions from users.\n",
    "\n",
    "In our case, we are going to index some LangSmith documentation!\n",
    "\n",
    "LangSmith makes it easy to trace any LLM application, no LangChain required!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2IUZy6SC2wj8"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCh1v9Qz2wj8"
   },
   "source": [
    "Make sure you set your environment variables, including your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 27518,
     "status": "ok",
     "timestamp": 1756608936977,
     "user": {
      "displayName": "Chaoran Zhou",
      "userId": "09162553986537566448"
     },
     "user_tz": -480
    },
    "id": "pOIAnY-oGxnX"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet -U langchain-google-genai langgraph langgraph-sdk langgraph-checkpoint-sqlite langsmith langchain-community langchain-core\n",
    "!pip install --quiet notebook python-dotenv lxml scikit-learn pandas pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 10640,
     "status": "ok",
     "timestamp": 1756608947596,
     "user": {
      "displayName": "Chaoran Zhou",
      "userId": "09162553986537566448"
     },
     "user_tz": -480
    },
    "id": "n9k9ZKUaHA5l"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1756608947629,
     "user": {
      "displayName": "Chaoran Zhou",
      "userId": "09162553986537566448"
     },
     "user_tz": -480
    },
    "id": "nB4-CZXlHAbB",
    "outputId": "198e6c97-4cfe-4fca-a86d-374413e9c2f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 227,
     "status": "ok",
     "timestamp": 1756608947859,
     "user": {
      "displayName": "Chaoran Zhou",
      "userId": "09162553986537566448"
     },
     "user_tz": -480
    },
    "id": "vuw_KweG2wj9"
   },
   "outputs": [],
   "source": [
    "# You can set them inline!\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1756608948067,
     "user": {
      "displayName": "Chaoran Zhou",
      "userId": "09162553986537566448"
     },
     "user_tz": -480
    },
    "id": "VmPolFiT2wj-",
    "outputId": "c8fe16c7-cafd-432f-a90d-968c43072cb2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BxtVqAO2wj-"
   },
   "source": [
    "### Simple RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2448,
     "status": "ok",
     "timestamp": 1756609500958,
     "user": {
      "displayName": "Chaoran Zhou",
      "userId": "09162553986537566448"
     },
     "user_tz": -480
    },
    "id": "0Hyv-cKp2wj-",
    "outputId": "d49b26d2-f2b6-4b84-dee8-d6a715dee93b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing vector store from: /tmp/union_local.parquet\n"
     ]
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "# from openai import OpenAI\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "import os\n",
    "from google import genai\n",
    "\n",
    "# MODEL_PROVIDER = \"openai\"\n",
    "# MODEL_NAME = \"gpt-4o-mini\"\n",
    "MODEL_NAME = \"gemini-2.5-flash\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "# openai_client = OpenAI()\n",
    "client = genai.Client(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "\"\"\"\n",
    "retrieve_documents\n",
    "- Returns documents fetched from a vectorstore based on the user's question\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "\"\"\"\n",
    "generate_response\n",
    "- Calls `call_gemini` to generate a model response after formatting inputs\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"parts\": [\n",
    "                {\"text\": RAG_SYSTEM_PROMPT},\n",
    "                {\"text\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    return call_gemini(messages)\n",
    "\n",
    "\"\"\"\n",
    "call_gemini\n",
    "- Returns the chat completion output from OpenAI\n",
    "\"\"\"\n",
    "@traceable(run_type=\"llm\")\n",
    "def call_gemini(\n",
    "    messages: List[dict], model: str = MODEL_NAME) -> str:\n",
    "\n",
    "    # return openai_client.chat.completions.create(\n",
    "    #     model=model,\n",
    "    #     messages=messages,\n",
    "    #     temperature=temperature,\n",
    "    # )\n",
    "\n",
    "    return client.models.generate_content(\n",
    "    model=model, contents=messages\n",
    "    )\n",
    "\n",
    "\"\"\"\n",
    "langsmith_rag\n",
    "- Calls `retrieve_documents` to fetch documents\n",
    "- Calls `generate_response` to generate a response based on the fetched documents\n",
    "- Returns the model response\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    # Access the text content from the response\n",
    "    return response.candidates[0].content.parts[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSOFipW92wj_"
   },
   "source": [
    "This should take a little less than a minute. We are indexing and storing LangSmith documentation in a SKLearn vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1086,
     "status": "ok",
     "timestamp": 1756609504499,
     "user": {
      "displayName": "Chaoran Zhou",
      "userId": "09162553986537566448"
     },
     "user_tz": -480
    },
    "id": "zpqi93lL2wj_",
    "outputId": "466fb757-f38f-416d-c3aa-d2fad3fd639b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith is used for storing and processing trace data and feedback. It utilizes Clickhouse as its primary storage engine for this purpose. LangSmith can be run via Kubernetes or Docker in a cloud environment.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is LangSmith used for?\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jB-iuSX2wkA"
   },
   "source": [
    "### Let's take a look in LangSmith!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ls-academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
