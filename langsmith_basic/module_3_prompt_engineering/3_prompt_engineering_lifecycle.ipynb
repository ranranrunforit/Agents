{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMQNzTF8kaj2"
   },
   "source": [
    "# Prompt Engineering Lifecycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZSiof4ikaj4"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29888,
     "status": "ok",
     "timestamp": 1756791973847,
     "user": {
      "displayName": "Chaoran Zhou",
      "userId": "09162553986537566448"
     },
     "user_tz": -480
    },
    "id": "pOIAnY-oGxnX",
    "outputId": "4c3fbef1-b55f-4cde-de60-4e80c1bc80e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.5/378.5 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.0/444.0 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.6/151.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
      "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install --quiet -U langchain-google-genai langgraph langgraph-sdk langgraph-checkpoint-sqlite langsmith langchain-community langchain-core\n",
    "!pip install --quiet notebook python-dotenv lxml scikit-learn pandas pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 7334,
     "status": "ok",
     "timestamp": 1756792601296,
     "user": {
      "displayName": "Chaoran Zhou",
      "userId": "09162553986537566448"
     },
     "user_tz": -480
    },
    "id": "n9k9ZKUaHA5l"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 261,
     "status": "ok",
     "timestamp": 1756792601567,
     "user": {
      "displayName": "Chaoran Zhou",
      "userId": "09162553986537566448"
     },
     "user_tz": -480
    },
    "id": "nB4-CZXlHAbB",
    "outputId": "041aab24-91a7-4f26-9686-8dc2985cc179"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGBFIXhAkaj5"
   },
   "outputs": [],
   "source": [
    "# You can set them inline\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJzJeK_5kaj7"
   },
   "outputs": [],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWAK7APrkaj7"
   },
   "source": [
    "### Log a trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 667,
     "referenced_widgets": [
      "5afd58f47eb84f85bc9815864c30727a",
      "41c15f30e3884d3eb56d2222231b74d8",
      "abe3bc483e964f00b4d980c8c8f6ad89",
      "a39194da1bcc4a62acb0e43cd531956f",
      "613b7296a5214635956bebcfa75205f7",
      "35c49d80f4544bca913afcf782d16911",
      "5c27b11a0fa64bbfaf9174718aca7257",
      "5fde4383a1f043cbbb94a88eea0eb1e3",
      "a8dc953240e54ddb8ed99075d682c4ca",
      "a517cc57ec7e4666bff95eedd273bf9b",
      "3572359e249a4112baeabed882fbeb3a",
      "adf0c510441a4bb9bd28037a9b3b2ce6",
      "5b0b0a7ded434794ae4cb9c53a136714",
      "189db5cada6e40bda003f747d7bf08ce",
      "208607c13f1e4ef3a54b9d9b0d5406ac",
      "c1a7a498341a4075a0277422646be611",
      "ace439d8d120409cba07962134681a0c",
      "8b85b28dbf0240c496576e0b39a2db90",
      "8fb34c5574674c2daea6b86ad391ddd2",
      "235bf8de2ac642e9a0bb8d48d45797cf",
      "09f71320ef2e4c438286fa69cd30eb1f",
      "14e0db125b674b29a3baebac14895998",
      "314032355c1249658910e20abb39adb1",
      "0377f3e7324f4db5bda151feafdd7962",
      "0ffbebf57eb44519b5656e9589b59520",
      "3d94d85a067541d29d4be61b083bc79e",
      "07e36edb16994a58bdd1fa492086fdf0",
      "c8265805bb2a437ea6e84ab84106f40c",
      "a6e9f1c186544e83bb67132d6c26de44",
      "956606947bdc457f9b984f5f4e573e9c",
      "537abd0c2d2845ec9546ed15e03a57b3",
      "85ba2dde0ddc4d5699cb09c519716811",
      "b097469a418c4cd7a7d6fd5d4fb8cc83",
      "8d8c9933dce340a484ccc3eee4915f2f",
      "9704e0b21a944870b5a08ed2eb06edee",
      "56c5c83dbf584ef089343951d607d376",
      "e2e9a6576bdd4a0582b73bc7a22c6586",
      "9cf2c5b5fc334baea6ad73129be776c3",
      "9c01493da30c4fc9aa3f5a5b82b0e108",
      "9986c3d01fd042afbb9f3bebcbcf9dc6",
      "55981de13577418c9c0ed73205825721",
      "ecea9a4c34fe4330a5196780a0c61c6d",
      "69e04d3363b641b082bef3bcbc5b8870",
      "4838cf3043314c3f9964b23939bca49c",
      "3f57d0008f774a75baf242f1b103d3d6",
      "c9719a283f5f4064bb0f471f65c7394c",
      "c4aa0908ae464ce598800c3fcac06f7d",
      "ad7061c750054f63b5e3af2df86c1cce",
      "2411c4f021064e05891a0064e8e42899",
      "c2fd9a4580734e9cb8a11a5531b1683e",
      "d8833b29045a470fbbf9a54f0366b404",
      "0deb690d14054f238ad7aa3a86c35d7f",
      "98dc3c7861334bb9978f193abcb75d8c",
      "d4b421c6a15e4c68afb647d6dd407cc7",
      "2febf77e61ba4908bb672e522a54119e",
      "4cc5e9a55f254a238dc89111c812e4d9",
      "6a26dcf305aa474ead22f1595ff6d851",
      "c45ea22bf2e64bc3b718316d5fc512d5",
      "83cb1ff73c6f49a4b845c54519872f47",
      "cae32904d755424ba076cec9a811ac8d",
      "03b23413e7304e05816aff3fd12a1a3c",
      "03db05fc0bb44883a1f9b233a5761abc",
      "1cb3ce4f9a934e1191efe76dc00114f7",
      "59c5327d48e34557af16085fe4d53235",
      "281517fe3ea9414fb730ee22c324421c",
      "af8e68e99da24684802e693dfc6508df",
      "29236ffee8ec45fea101735f2ca19b5e",
      "19aef0ea7b7b4558a6a2e47610583fc5",
      "eaefa7fb9cc3485db9ce74dba20ba627",
      "67622dc903a04d1ea3b77a377a28145c",
      "e54bd2879e3c4adfa62c721b26b29160",
      "eed6858835fc4d429f65dbf6a1685821",
      "753a9f4adb6943ac84194c9c0172b516",
      "2802a7685cad4c738d6e22b5fd6daca9",
      "fdd75971010b4d21ac285d9ac06374a9",
      "75f4ee1cc1c04a048cf152e81a501aac",
      "ab310e336c8544829592a4bc4bcce568",
      "695e4f1bd3534a958efe5185b6436707",
      "0af016dfd38d4d71b498343fe8bf8ba2",
      "8a8964fcaf1c4532bace07b1ce49fa01",
      "7ac226872cc64581ae81132c3e22aa24",
      "0cc7458b167f4120bc10b3562e15abf7",
      "e3d2938c817d4c6f853eaded9700a58a",
      "8a4908715b7b4cfe87ba28f85977b877",
      "6523645c28cb45849faabd2d54403bcd",
      "82830901819d436dbb22c1f360b7a80d",
      "49f378db8c6e430f9365eb5361d3ad1e",
      "f26d5ff57cb54e7d85b078f32ecb7de4",
      "be0ba094f82f45a38f38cd6cf0af6085",
      "f6d265d62bb64a23a18e239a84a43e6b",
      "e399ae2d26e040a88e50e0448c30d0df",
      "4cff7f295b6a4789b79c9a4ccf3b79d2",
      "2def553c991d43d5abc3425fcd36c332",
      "1ea8a68a3ed64cbcb49223dee5a962ce",
      "6f4ba21149c942cea06b48040be5b82a",
      "ffba41a69ac34fbc87dbd1c5c7f20021",
      "3784d84b3a0442b0a6668b11741156ad",
      "be3910a4c6794c75b7bbef61c969acb2",
      "591013caad81459eb2ea22d2a92761d0",
      "38adefe31a864a909f4f597e29d7178f",
      "0cec7c6310c840a881ddbf7c9a378feb",
      "65a789fecf994ab4a3bf9586b6a19ca8",
      "d3a472b381054b15926b49828783af43",
      "64941ea5b36442769701e70e0656a327",
      "9a93a9b1a26640c4a039ca8b50119a9d",
      "6b5e58c14fae43dcbc4c25b7020b2d86",
      "9ede2136a195459181777240b471373c",
      "8e62c6dea0f548af8826dfa6e863eb3d",
      "4b13d5832263451eb560b061ec5d80dd",
      "aa8acd5759014ec2818a8d503b70ba50",
      "5a0f0a366c38406cbd73ce69ac4e3f15",
      "de648bc7c6f8492e8c0c3e4445d281cc",
      "1c1c0c5475314124b687060d9926f3bd",
      "7671cd682fbc4f2e82fd49f4e39b9907",
      "e3af54ef59e1417a97e936d539ce578d",
      "4067e7e9740143d4bc8eefdd58c9d5f8",
      "34578f95898642c8b78a3cebc85d0da1",
      "a103e2e7bad84b98b0408eba0a3f5aae",
      "dca7471158384a518ac3de9445262c60",
      "a975d7414f594e4296a41528c2989488",
      "c5e155e105e54013ad49c44d3b13dbab"
     ]
    },
    "executionInfo": {
     "elapsed": 234779,
     "status": "ok",
     "timestamp": 1756792904873,
     "user": {
      "displayName": "Chaoran Zhou",
      "userId": "09162553986537566448"
     },
     "user_tz": -480
    },
    "id": "aqmPu1FEkaj8",
    "outputId": "98b46ba0-28b6-421b-f4a1-100f97373768"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "/content/drive/MyDrive/Colab Notebooks/intro-to-langsmith-main/notebooks/module_3_prompt_engineering/app.py:46: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embd = HuggingFaceEmbeddings(\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5afd58f47eb84f85bc9815864c30727a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf0c510441a4bb9bd28037a9b3b2ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314032355c1249658910e20abb39adb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8c9933dce340a484ccc3eee4915f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f57d0008f774a75baf242f1b103d3d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc5e9a55f254a238dc89111c812e4d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29236ffee8ec45fea101735f2ca19b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695e4f1bd3534a958efe5185b6436707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be0ba094f82f45a38f38cd6cf0af6085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38adefe31a864a909f4f597e29d7178f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0f0a366c38406cbd73ce69ac4e3f15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing vector store found. Indexing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 197/197 [00:26<00:00,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1159 document splits. Creating vector store...\n",
      "Persisting vector store to: /tmp/union_local.parquet\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"To set up tracing to LangSmith with `@traceable`, you need to decorate any function with `@traceable`. Additionally, the `LANGSMITH_TRACING` environment variable must be set to 'true', and the `LANGSMITH_API_KEY` environment variable must be set to your API key. These environment variables enable tracing and authenticate your requests to LangSmith.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from app import langsmith_rag\n",
    "\n",
    "question = \"How do I set up tracing to LangSmith with @traceable?\"\n",
    "langsmith_rag(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCtPt3Uikaj9"
   },
   "source": [
    "### Create a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NM4vUmAkaj9"
   },
   "source": [
    "Let's create a dataset to evaluate this particular step of our application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 899,
     "status": "ok",
     "timestamp": 1756792928427,
     "user": {
      "displayName": "Chaoran Zhou",
      "userId": "09162553986537566448"
     },
     "user_tz": -480
    },
    "id": "eBFDvKWrkaj-",
    "outputId": "5c57bc70-8b74-484e-9f5e-9195dd14e195"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['fe72906e-b891-4ab1-8481-aff782cbc230',\n",
       "  '41d122d4-006b-4c8e-8663-c3e1d2700d02',\n",
       "  '4db5cd63-088d-4939-9619-1e9068aa1b7d'],\n",
       " 'count': 3}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "example_dataset = [\n",
    "    (\n",
    "        \"How do I set up tracing to LangSmith with @traceable?\",\n",
    "        \"\"\" 2. Log a trace​\\nOnce you've set up your environment, you can call LangChain runnables as normal.\\nLangSmith will infer the proper tracing config:\\n\\nHow to log and view traces to LangSmith | 🦜️🛠️ LangSmith\\n\\n2. Log a trace​\\nOnce you've set up your environment, wrap or decorate the custom functions/SDKs you want to trace.\\nLangSmith will then infer the proper tracing config:\\n\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startObservabilityConceptual GuideHow-to GuidesTracingAnnotate code for tracingToggle tracing on and offUpload files with tracesLog traces to specific projectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesPrevent logging of sensitive data in tracesQuery tracesShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChain (Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APICalculate token-based costs for tracesTroubleshoot trace nesting[Beta] Bulk Exporting Trace DataTrace JS functions in serverless environmentsMonitoring and automationsTutorialsAdd observability to your LLM applicationEvaluationPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceObservabilityHow-to GuidesTracingAnnotate code for tracingOn this pageAnnotate code for tracing\\nThere are several ways to log traces to LangSmith.\\ntipIf you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the LangChain-specific instructions.\\nUse @traceable / traceable​\\nLangSmith makes it easy to log traces with minimal changes to your existing code with the @traceable decorator in Python and traceable function in TypeScript.\\nnoteThe LANGSMITH_TRACING environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using @traceable or traceable. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGSMITH_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.\\nTo log traces to a different project, see this section. \\n\\n\"\"\",\n",
    "        \"To set up tracing to LangSmith using the `@traceable` decorator in Python, first ensure that you have the `LANGSMITH_TRACING` environment variable set to 'true' and the `LANGSMITH_API_KEY` set to your API key. Then, you can simply decorate your functions like this:\\n\\n```python\\nfrom langsmith import traceable\\n\\n@traceable\\ndef my_function():\\n    # Your code here\\n    pass\\n```\\n\\nThis will log traces for `my_function` automatically once the necessary environment variables are configured.\"\n",
    "    ),\n",
    "    (\n",
    "        \"How can I use the tracing context manager?\",\n",
    "        \"\"\"Use the trace context manager (Python only)​\\nIn Python, you can use the trace context manager to log traces to LangSmith. This is useful in situations where:\\n\\nYou want to log traces for a specific block of code.\\nYou want control over the inputs, outputs, and other attributes of the trace.\\nIt is not feasible to use a decorator or wrapper.\\nAny or all of the above.\\n\\nIn some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically.\\nRecently changed behaviorDue to a number of asks for finer-grained control of tracing using the trace context manager,\\nwe changed the behavior of with trace to honor the LANGSMITH_TRACING environment variable in version 0.1.95 of the Python SDK. You can find more details in the release notes.\\nThe recommended way to disable/enable tracing without setting environment variables is to use the with tracing_context context manager, as shown in the example below.\\nPythonTypeScriptThe recommended way to do this in Python is to use the tracing_context context manager. This works for both code annotated with traceable and code within the trace context manager.\\n\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startObservabilityConceptual GuideHow-to GuidesTracingAnnotate code for tracingToggle tracing on and offUpload files with tracesLog traces to specific projectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesPrevent logging of sensitive data in tracesQuery tracesShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChain (Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APICalculate token-based costs for tracesTroubleshoot trace nesting[Beta] Bulk Exporting Trace DataTrace JS functions in serverless environmentsMonitoring and automationsTutorialsAdd observability to your LLM applicationEvaluationPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceObservabilityHow-to GuidesTracingTroubleshoot trace nestingOn this pageTroubleshoot trace nesting\\nWhen tracing with the LangSmith SDK, LangGraph, and LangChain, tracing should automatically propagate the correct context so that code executed within a parent trace will be rendered in the expected location in the UI.\\nIf you see a child run go to a separate trace (and appear on the top level), it may be caused by one of the following known \\\"edge cases\\\".\\nPython​\\nThe following outlines common causes for \\\"split\\\" traces when building with python.\\nContext propagation using asyncio​\\nWhen using async calls (especially with streaming) in Python versions < 3.11, you may encounter issues with trace nesting. This is because Python's asyncio only added full support for passing context in version 3.11.\\nWhy​\\nLangChain and LangSmith SDK use contextvars to propagate tracing information implicitly. In Python 3.11 and above, this works seamlessly. However, in earlier versions (3.8, 3.9, 3.10), asyncio tasks lack proper contextvar support, which can lead to disconnected traces.\\nTo resolve​\\n\\nContext propagation using threading​\\nIt's common to start tracing and want to apply some parallelism on child tasks all within a single trace. Python's stdlib ThreadPoolExecutor by default breaks tracing.\\nWhy​\\nPython's contextvars start empty within new threads. Here are two approaches to handle maintain trace contiguity:\\nTo resolve​\\n\\n\\nUsing LangSmith's ContextThreadPoolExecutor\\nLangSmith provides a ContextThreadPoolExecutor that automatically handles context propagation:\\nfrom langsmith.utils import ContextThreadPoolExecutorfrom langsmith import traceable@traceabledef outer_func():    with ContextThreadPoolExecutor() as executor:        inputs = [1, 2]        r = list(executor.map(inner_func, inputs))@traceabledef inner_func(x):    print(x)outer_func()\\n\\n\\nManually providing the parent run tree\\nAlternatively, you can manually pass the parent run tree to the inner function:\\nfrom langsmith import traceable, get_current_run_treefrom concurrent.futures import ThreadPoolExecutor@traceabledef outer_func():    rt = get_current_run_tree()    with ThreadPoolExecutor() as executor:        r = list(            executor.map(                lambda x: inner_func(x, langsmith_extra={\\\"parent\\\": rt}), [1, 2]            )        )@traceabledef inner_func(x):    print(x)outer_func()\\nIn this approach, we use get_current_run_tree() to obtain the current run tree and pass it to the inner function using the langsmith_extra parameter. \\n\\n \"\"\",\n",
    "        \"You can use the tracing context manager by wrapping the code you want to trace with the `with trace_context:` statement. Here's a simple example:\\n\\n```python\\nfrom langsmith import trace_context\\n\\nwith trace_context:\\n    # Your traceable code here\\n    print(\\\"Tracing this block of code.\\\")\\n``` \\n\\nThis will log traces to LangSmith for the specified code block.\"\n",
    "    ),\n",
    "    (\n",
    "        \"How can I use RunTree?\",\n",
    "        \"\"\"PythonTypeScriptimport openaifrom langsmith.run_trees import RunTree# This can be a user input to your appquestion = \\\"Can you summarize this morning's meetings?\\\"# Create a top-level runpipeline = RunTree(  name=\\\"Chat Pipeline\\\",  run_type=\\\"chain\\\",  inputs={\\\"question\\\": question})# This can be retrieved in a retrieval stepcontext = \\\"During this morning's meeting, we solved all world conflict.\\\"messages = [  { \\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a helpful assistant. Please respond to the user's request only based on the given context.\\\" },  { \\\"role\\\": \\\"user\\\", \\\"content\\\": f\\\"Question: {question}\\\\nContext: {context}\\\"}]# Create a child runchild_llm_run = pipeline.create_child(  name=\\\"OpenAI Call\\\",  run_type=\\\"llm\\\",  inputs={\\\"messages\\\": messages},)# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create(  model=\\\"gpt-4o-mini\\\", messages=messages)# End the runs and log themchild_llm_run.end(outputs=chat_completion)child_llm_run.postRun()pipeline.end(outputs={\\\"answer\\\": chat_completion.choices[0].message.content})pipeline.postRun()import OpenAI from \\\"openai\\\";import { RunTree } from \\\"langsmith\\\";// This can be a user input to your appconst question = \\\"Can you summarize this morning's meetings?\\\";const pipeline = new RunTree({  name: \\\"Chat Pipeline\\\",  run_type: \\\"chain\\\",  inputs: { question }});await pipeline.postRun();// This can be retrieved in a retrieval stepconst context = \\\"During this morning's meeting, we solved all world conflict.\\\";const messages = [  { role: \\\"system\\\", content: \\\"You are a helpful assistant. Please respond to the user's request only based on the given context.\\\" },  { role: \\\"user\\\", content: `Question: ${question}Context: ${context}` }];// Create a child runconst childRun = await pipeline.createChild({  name: \\\"OpenAI Call\\\",\\n\\nPythonTypeScriptimport openaifrom langsmith.run_trees import RunTree# This can be a user input to your appquestion = \\\"Can you summarize this morning's meetings?\\\"# Create a top-level runpipeline = RunTree(  name=\\\"Chat Pipeline\\\",  run_type=\\\"chain\\\",  inputs={\\\"question\\\": question})await pipeline.postRun();# This can be retrieved in a retrieval stepcontext = \\\"During this morning's meeting, we solved all world conflict.\\\"messages = [  { \\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a helpful assistant. Please respond to the user's request only based on the given context.\\\" },  { \\\"role\\\": \\\"user\\\", \\\"content\\\": f\\\"Question: {question}\\\\nContext: {context}\\\"}]# Create a child runchild_llm_run = pipeline.create_child(  name=\\\"OpenAI Call\\\",  run_type=\\\"llm\\\",  inputs={\\\"messages\\\": messages},)# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create(  model=\\\"gpt-3.5-turbo\\\", messages=messages)# End the runs and log themchild_llm_run.end(outputs=chat_completion)child_llm_run.postRun()pipeline.end(outputs={\\\"answer\\\": chat_completion.choices[0].message.content})pipeline.postRun()import OpenAI from \\\"openai\\\";import { RunTree } from \\\"langsmith\\\";// This can be a user input to your appconst question = \\\"Can you summarize this morning's meetings?\\\";const pipeline = new RunTree({  name: \\\"Chat Pipeline\\\",  run_type: \\\"chain\\\",  inputs: { question }});// This can be retrieved in a retrieval stepconst context = \\\"During this morning's meeting, we solved all world conflict.\\\";const messages = [  { role: \\\"system\\\", content: \\\"You are a helpful assistant. Please respond to the user's request only based on the given context.\\\" },  { role: \\\"user\\\", content: `Question: ${question}Context: ${context}` }];// Create a child runconst childRun = await pipeline.createChild({  name: \\\"OpenAI\\n\\nAlternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function.\\n\\nRunTree({  run_type: \\\"llm\\\",  name: \\\"OpenAI Call RunTree\\\",  inputs: { messages },  tags: [\\\"my-tag\\\"],  extra: {metadata: {\\\"my-key\\\": \\\"my-value\\\"}}})await rt.postRun();const chatCompletion = await client.chat.completions.create({  model: \\\"gpt-3.5-turbo\\\",  messages: messages,});// End and submit the runawait rt.end(chatCompletion)await rt.patchRun()from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([(\\\"system\\\", \\\"You are a helpful AI.\\\"),(\\\"user\\\", \\\"{input}\\\")])chat_model = ChatOpenAI()output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({\\\"tags\\\": [\\\"top-level-tag\\\"], \\\"metadata\\\": {\\\"top-level-key\\\": \\\"top-level-value\\\"}})# Tags and metadata can also be passed at runtimechain.invoke({\\\"input\\\": \\\"What is the meaning of life?\\\"}, {\\\"tags\\\": [\\\"shared-tags\\\"], \\\"metadata\\\": {\\\"shared-key\\\": \\\"shared-value\\\"}})import { ChatOpenAI } from \\\"@langchain/openai\\\";import { ChatPromptTemplate } from \\\"@langchain/core/prompts\\\";import { StringOutputParser } from \\\"@langchain/core/output_parsers\\\";const prompt = ChatPromptTemplate.fromMessages([[\\\"system\\\", \\\"You are a helpful AI.\\\"],[\\\"user\\\", \\\"{input}\\\"]])const model = new ChatOpenAI({ modelName: \\\"gpt-3.5-turbo\\\" });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({\\\"tags\\\": [\\\"top-level-tag\\\"], \\\"metadata\\\": {\\\"top-level-key\\\": \\\"top-level-value\\\"}});// Tags and metadata can also be \\n\\n\"\"\",\n",
    "        \"You can use `RunTree` by creating a pipeline where you define its name, type, and inputs. For example:\\n\\n```python\\nfrom langsmith.run_trees import RunTree\\n\\npipeline = RunTree(name=\\\"My Pipeline\\\", run_type=\\\"chain\\\", inputs={\\\"question\\\": \\\"Your question here\\\"})\\n```\\n\\nYou can then create child runs and manage them accordingly within the pipeline.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "ls_client = Client()\n",
    "dataset_name = \"Technical Questions\"\n",
    "\n",
    "# Create dataset\n",
    "dataset = ls_client.create_dataset(\n",
    "    dataset_name=dataset_name, description=\"Technical questions about LangSmith\"\n",
    ")\n",
    "\n",
    "# Prepare inputs and outputs\n",
    "inputs = [{\"question\": q, \"context\": c} for q, c, _ in example_dataset]\n",
    "outputs = [{\"output\": o} for _, _, o in example_dataset]\n",
    "\n",
    "# Create examples in the dataset\n",
    "ls_client.create_examples(\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    dataset_id=dataset.id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OjCT1-D5kaj_"
   },
   "source": [
    "### Update our Application to use Prompt Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoZVFhr1kakA"
   },
   "source": [
    "We're going to pretty much define the same RAG application as before - with one crucial improvement.\n",
    "\n",
    "Instead of pulling our `RAG_PROMPT` from utils.py, we're going to connect to the Prompt Hub in LangSmith.\n",
    "\n",
    "Let's add the code snippet that will pull down our prompt that we just iterated on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1756794870475,
     "user": {
      "displayName": "Chaoran Zhou",
      "userId": "09162553986537566448"
     },
     "user_tz": -480
    },
    "id": "xWwhrHwRGkj-"
   },
   "outputs": [],
   "source": [
    "# Create a LANGSMITH_API_KEY in Settings > API Keys\n",
    "from langsmith import Client\n",
    "ls_client = Client()\n",
    "prompt = ls_client.pull_prompt(\"rag_with_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1034,
     "status": "ok",
     "timestamp": 1756794881552,
     "user": {
      "displayName": "Chaoran Zhou",
      "userId": "09162553986537566448"
     },
     "user_tz": -480
    },
    "id": "-cvXPyqLHClU",
    "outputId": "da9a3b29-9fb6-4bdc-f238-1c554b2b88fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing vector store from: /tmp/union_local.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.sitemap import SitemapLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "#from langchain_openai import OpenAIEmbeddings\n",
    "from langsmith import traceable\n",
    "#from openai import OpenAI\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "\n",
    "MODEL_NAME = \"gemini-2.5-flash-lite\"\n",
    "MODEL_PROVIDER = \"google\"\n",
    "APP_VERSION = 1.0\n",
    "# TODO: Remove this hard-coded prompt and replace it with Prompt Hub\n",
    "\n",
    "\n",
    "import os\n",
    "from google import genai\n",
    "# openai_client = OpenAI()\n",
    "g_client = genai.Client(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def get_vector_db_retriever():\n",
    "    \"\"\"\n",
    "    Creates or loads a scikit-learn based vector store retriever.\n",
    "\n",
    "    This function replaces OpenAIEmbeddings with a local, open-source model\n",
    "    from Hugging Face (sentence-transformers/all-MiniLM-L6-v2) for generating\n",
    "    document embeddings.\n",
    "\n",
    "    Returns:\n",
    "        A retriever object for querying the vector store.\n",
    "    \"\"\"\n",
    "    persist_path = os.path.join(tempfile.gettempdir(), \"union_local.parquet\")\n",
    "\n",
    "    # Initialize a local, open-source embedding model from Hugging Face.\n",
    "    # This model runs on your machine and does not require an API key.\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    model_kwargs = {'device': 'cpu'} # Use CPU for embedding\n",
    "    encode_kwargs = {'normalize_embeddings': False}\n",
    "    embd = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "\n",
    "    # If vector store exists, then load it\n",
    "    if os.path.exists(persist_path):\n",
    "        print(f\"Loading existing vector store from: {persist_path}\")\n",
    "        vectorstore = SKLearnVectorStore(\n",
    "            embedding=embd,\n",
    "            persist_path=persist_path,\n",
    "            serializer=\"parquet\"\n",
    "        )\n",
    "        # lambda_mult=0 is used for Maximal Marginal Relevance (MMR) search.\n",
    "        # It effectively disables MMR and performs a standard similarity search.\n",
    "        return vectorstore.as_retriever(lambda_mult=0)\n",
    "\n",
    "    # Otherwise, index LangSmith documents and create a new vector store\n",
    "    print(\"No existing vector store found. Indexing documents...\")\n",
    "    ls_docs_sitemap_loader = SitemapLoader(\n",
    "        web_path=\"https://docs.smith.langchain.com/sitemap.xml\",\n",
    "        continue_on_failure=True,\n",
    "        # Optional: Filter URLs to only include relevant documentation pages\n",
    "        # filter_urls=[\"https://docs.smith.langchain.com/\"]\n",
    "    )\n",
    "\n",
    "    # Set a custom user-agent to be respectful when scraping\n",
    "    ls_docs_sitemap_loader.headers = {\n",
    "        \"User-Agent\": \"LocalVectorDBBuilder/1.0 (https://example.com/bot-info)\"\n",
    "    }\n",
    "    ls_docs = ls_docs_sitemap_loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=500, chunk_overlap=100 # Increased overlap for better context\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(ls_docs)\n",
    "\n",
    "    print(f\"Created {len(doc_splits)} document splits. Creating vector store...\")\n",
    "\n",
    "    vectorstore = SKLearnVectorStore.from_documents(\n",
    "        documents=doc_splits,\n",
    "        embedding=embd,\n",
    "        persist_path=persist_path,\n",
    "        serializer=\"parquet\"\n",
    "    )\n",
    "\n",
    "    print(f\"Persisting vector store to: {persist_path}\")\n",
    "    vectorstore.persist()\n",
    "    return vectorstore.as_retriever(lambda_mult=0)\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "\"\"\"\n",
    "retrieve_documents\n",
    "- Returns documents fetched from a vectorstore based on the user's question\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "\"\"\"\n",
    "generate_response\n",
    "- Calls `call_openai` to generate a model response after formatting inputs\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    # TODO: Let's use our prompt pulled from Prompt Hub instead of manually formatting here!\n",
    "    formatted_prompt = prompt.invoke({\"context\":formatted_docs, \"question\": question})\n",
    "    # messages = convert_prompt_to_openai_format(formatted_prompt)[\"messages\"]\n",
    "    gemini_contents = []\n",
    "    for message in formatted_prompt.messages:\n",
    "        gemini_contents.append({\"text\": message.content})\n",
    "    return call_gemini(gemini_contents)\n",
    "\n",
    "\"\"\"\n",
    "call_gemini\n",
    "- Returns the chat completion output from OpenAI\n",
    "\"\"\"\n",
    "@traceable(\n",
    "    run_type=\"llm\",\n",
    "    metadata={\n",
    "        \"ls_provider\": MODEL_PROVIDER,\n",
    "        \"ls_model_name\": MODEL_NAME\n",
    "    }\n",
    ")\n",
    "def call_gemini(messages: List[dict]) -> str:\n",
    "    # return openai_client.chat.completions.create(\n",
    "    #     model=MODEL_NAME,\n",
    "    #     messages=messages,\n",
    "    # )\n",
    "    return g_client.models.generate_content(\n",
    "    model=MODEL_NAME, contents=messages\n",
    "    )\n",
    "\n",
    "\"\"\"\n",
    "langsmith_rag\n",
    "- Calls `retrieve_documents` to fetch documents\n",
    "- Calls `generate_response` to generate a response based on the fetched documents\n",
    "- Returns the model response\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    # return response.choices[0].message.content\n",
    "    return response.candidates[0].content.parts[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "executionInfo": {
     "elapsed": 881,
     "status": "ok",
     "timestamp": 1756794885676,
     "user": {
      "displayName": "Chaoran Zhou",
      "userId": "09162553986537566448"
     },
     "user_tz": -480
    },
    "id": "7_6xo58qIUN-",
    "outputId": "29396558-eb6c-48c0-e01d-bb438c99915a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"To set up tracing to LangSmith with `@traceable`, you need to set the `LANGSMITH_TRACING` environment variable to `'true'` and the `LANGSMITH_API_KEY` environment variable to your API key. You can then decorate any function with `@traceable` to log its traces.\\n\\n```python\\nfrom langsmith import traceable\\nfrom openai import Client\\n\\n@traceable\\ndef my_function():\\n    # Your function logic here\\n    pass\\n\\n# Example of calling a sync function wrapped with traceable\\n# @traceable\\n# def formatPrompt():\\n#     pass\\n# await formatPrompt()\\n```\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How do I set up tracing to LangSmith with @traceable?\"\n",
    "langsmith_rag(question)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ls-academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
