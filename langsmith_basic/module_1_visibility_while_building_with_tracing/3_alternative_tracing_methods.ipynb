{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsnH6K4EBueV"
   },
   "source": [
    "# Alternative Tracing Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlvn0-0ABueZ"
   },
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJGllWnuBuec"
   },
   "source": [
    "So far in this module, we've taken a look at the traceable decorator, and how we can use it to set up tracing.\n",
    "\n",
    "In this lesson, we're going to look at alternative ways in which we can set up tracing, and when you should think about using these different approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uk2J5VViBued"
   },
   "source": [
    "## LangChain and LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CNcYUTuBued"
   },
   "source": [
    "If we are using LangChain or LangGraph, all we need to do to set up tracing is to set a few environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6SVgIvEBuee"
   },
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45400,
     "status": "ok",
     "timestamp": 1756694866852,
     "user": {
      "displayName": "Chaoran Zhou",
      "userId": "09162553986537566448"
     },
     "user_tz": -480
    },
    "id": "MrG5vvjFGgOG",
    "outputId": "bbec6103-4879-4b04-9495-d3bef16ef264"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/My Drive/Colab Notebooks/intro-to-langsmith-main/notebooks/module_1\n"
     ]
    }
   ],
   "source": [
    "### Mount Notebook to Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# change the working directory to the Drive root\n",
    "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/intro-to-langsmith-main/notebooks/module_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36516,
     "status": "ok",
     "timestamp": 1756694906736,
     "user": {
      "displayName": "Chaoran Zhou",
      "userId": "09162553986537566448"
     },
     "user_tz": -480
    },
    "id": "pOIAnY-oGxnX",
    "outputId": "de3f87ca-a419-4ad6-ea5f-837f8a93fca6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.5/378.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.0/444.0 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.6/151.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
      "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install --quiet -U langchain-google-genai langgraph langgraph-sdk langgraph-checkpoint-sqlite langsmith langchain-community langchain-core\n",
    "!pip install --quiet notebook python-dotenv lxml scikit-learn pandas pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 15341,
     "status": "ok",
     "timestamp": 1756694924647,
     "user": {
      "displayName": "Chaoran Zhou",
      "userId": "09162553986537566448"
     },
     "user_tz": -480
    },
    "id": "n9k9ZKUaHA5l"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 837,
     "status": "ok",
     "timestamp": 1756694929258,
     "user": {
      "displayName": "Chaoran Zhou",
      "userId": "09162553986537566448"
     },
     "user_tz": -480
    },
    "id": "nB4-CZXlHAbB",
    "outputId": "09b92703-3676-49c1-8b22-50176e6a5e3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTCsTuDhBuee"
   },
   "outputs": [],
   "source": [
    "# You can set them inline\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\"  # If you don't set this, traces will go to the Default project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUSfqV-JBuef"
   },
   "outputs": [],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lho7BESRBuef"
   },
   "source": [
    "Don't worry too much about our graph implementation here, you can learn more about LangGraph through our LangGraph Academy course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 948,
     "referenced_widgets": [
      "ec2f96b7deaf4de899744acbbe447c65",
      "b360f5a0c59d4cce8742b49c7ce1c344",
      "586d620b42184822a1d0c1d889423d29",
      "c1c03e7b738342ed82ad4ce6c4af0871",
      "8b776dca4ca1405285d902ccf59b14d1",
      "1e62d278ec354683bdd65d8c259fdaa2",
      "6fbd72a245e9480eaa5a93577a1bb90a",
      "19cf88e14af94b52bedd5bcb925ac6da",
      "433251dafce6467cad8b0ad5584d42cb",
      "61e9b797d9224400ad8b7b2baf3550f4",
      "4cd910fc74cb45dfa56a3d4e006d2177",
      "780830b4c0a741cebf86ce203be19dab",
      "86d672591ae54944845f839e32b4473b",
      "f6d45c44b5c146cab5ee51c061299da1",
      "954be02b53da418f892b1d651bd33270",
      "b8c829628a14410391aeeea6b658d8c8",
      "24b7fef476b74cd692879ec59ac20b74",
      "252c56b18e494ea4a1680025afccca9e",
      "95a310c4cc1d4dbe8ebe0afd593a6fd3",
      "1b92451ccdb8402fa8c2153398aedb4b",
      "971a8b5e40f64f608b81a2586bb08937",
      "4ef3a2e9f0ed47e2954444db7f86e70b",
      "efd2ba51412240deb7064339d45ff875",
      "6bda848f6164427aa54cf4b0b33c0ee0",
      "2630c13fa337428983651604f8c00fe5",
      "e4b09bd6af9344749e7bcd6a2eb0d3b7",
      "366b8f8e471b4d64b9f058b8412a4430",
      "f7e2978c954f4278a9e8c8422aa5f2e7",
      "a5d0f6f67eb44c7da0329195910aa6ff",
      "1fd3b8310ae546db980d37826e961c46",
      "809b7d8720d34a95a88a14cef23483bf",
      "c88f53d8b2dc426383292edab4205a80",
      "5b66452028ea43d59138e7a1efb9a54a",
      "c77b042796b448c5a2b83962c10622e5",
      "a59616e3b0984f25a243748f3a589151",
      "f053443726b1405393febb7c4a249360",
      "e1d89a119f5149d99d91f9e406af82a2",
      "591b444f3c9a46e595784c66bf872c1e",
      "60e5b8edc83c496c96d2c9e588d8caa5",
      "b6482aca561f4e7f870918afeecf5bc1",
      "7c8e034e01064f669bb26a61d2f904f4",
      "5041457842b24487b65e8557b259b653",
      "c4f789ea88c44f68bf590657d0ea5433",
      "92115809c733415cbb75dc7b30199319",
      "9476fafbbe5c4de99a724cd892427426",
      "c677005601c64ea48e76452a3988a6d5",
      "c1d58c35c27644afb0eacdb75c7a047f",
      "92bc08aaec904da4ad4eda79008d5bc7",
      "fb10634fc8bb44e4ab15fb90a7854594",
      "195d3c2fc0b24379a7a607f3ef7b2330",
      "f85a8ef4c9234bac864d8c085f32d4f1",
      "07ce8a671285417ea4866b4ae8ba69f7",
      "814524916a724dbbb2522a2773b3bf44",
      "438d99078a904bde9a90c245201d399a",
      "527ec837f0204339b4ea1dc5d55432ed",
      "b1e065052e2844dfbfe15f69c62acf80",
      "fa995eebdf7a42ea85d0823105b7e5a2",
      "8cf66fc8b6cb4d759ff15996f98b2251",
      "52c9c48943984d5dbc84cbb6ce1d86a8",
      "1e81a65fdb3846e798b0efc59382d362",
      "4df4499d23094dfd99984a4b5c08db9a",
      "c8a8140395f4481aa0a5af10fc749e4c",
      "977177a0594b4a22a629fee1588e0cd3",
      "111b27c6ef9d43cc9d9d06f01e5bb8b1",
      "1853908d9dfb4fa69b7c9da6e16f097b",
      "1c170b028829481699f498f70b91cda8",
      "80fbfd698477400a8e08930c04815efa",
      "f34e968702104212971a6dfaabc0e587",
      "6d914dfac95b415186de26a50044b038",
      "67d7a3584f584baeb8effd6360ea9902",
      "b8f37610f70344d5867dab5e22416cfa",
      "e5505845965349c98d3a2addeab5e428",
      "b5f3dea45dc4441f9274f2d8c9267c00",
      "b2b0c2dd890a4f4fb8e6e675f7ed5571",
      "20c355c6f8974162ae601b99d5b6be51",
      "cae774a0fc8c450881f6af4f48198479",
      "3fa9aeb44a2745aca261de1c0b95c1ff",
      "89da5cd3395e491b8326fd89715e4cb1",
      "cae8a3e9a0eb4e82a0e358f0ee43ae0f",
      "677eb14e89a643049d85a8ab83aacbb5",
      "1dfd88b89bc9409f8d6d46083645406a",
      "03eb9877da1c4a748a769c5ef41430a2",
      "11ec12ec16eb4494b9be2c11bdf87705",
      "22566ae839534971a7b6e1492ca46bec",
      "2a8c8d9eb37b4c62aa661a078baa4511",
      "631ff5760d06483d92814cea3bdcbfc4",
      "42151653cc964fda87145ae4c06348db",
      "84934648946040e6ab46a54d13c56822",
      "43a371117173461b86dd3be5448bea1c",
      "826f88c18fee47838f525851bec5b531",
      "2b6d2ec7e04a44de87fc761f481a4041",
      "f77b6c8604594bf38ffcdc5e47a9f0be",
      "60bc86d4878348258e33bf63f79c4b7a",
      "60058cab8bf548ca839442de20d52fe5",
      "4365b978580044c0aa720ddcf995022d",
      "14c4455ce6204cfcaf9df5b7e53bd1b7",
      "cadb4f55033c46e8bdce7ff4c20a6522",
      "f03ce1b818524b3aaf4ab3cadc7b92ce",
      "8ade9aed4c3c41ceb9d67a35a01e734d",
      "e743557d8853499dbedcfefc3e039616",
      "4bcc55044283410ea145de9c0f5709bb",
      "fc2d6db07cbb4bddb0364319d5acc6dd",
      "10ee9a4c71c34ae387bc03783f2fefd7",
      "a170a579d6a94abea0c79040cd4eb71b",
      "2f54ba8947b148e786c1f4c19f315938",
      "2a44a00117664ffca962ae13857a4aea",
      "ff1bb69917bb4204b85db13207b9affc",
      "8370bf4ef87b47768865181c71dc9a8f",
      "8db0aee652774320b5424dccb24a18b3",
      "86ec09c39e8044dca4fc810540b08213",
      "2d6b1ff6c74b4d6eae6ccd6882b57958",
      "bbd8749629774373832047c447861370",
      "918a393eb4024f45be34c96068a0e382",
      "e08b5403b39c4c23b1d2e6f756f7f5c3",
      "f15e8d1a04f4400eac9ed20890d8b250",
      "be6aa487753941fc811cc09eac021154",
      "0910b3b5132a4c6fb48acc83d24ab451",
      "799c5425ad4c4603949d1e7260026d02",
      "0f7131f3242949b580c9f0b09f51ece8",
      "9d6044dbfa4b485692b439105d32b0ba",
      "1ebe57289d3443e1883da0a171ef7dba"
     ]
    },
    "executionInfo": {
     "elapsed": 271831,
     "status": "ok",
     "timestamp": 1756695309505,
     "user": {
      "displayName": "Chaoran Zhou",
      "userId": "09162553986537566448"
     },
     "user_tz": -480
    },
    "id": "x75gvOMqBueh",
    "outputId": "709d03e8-2372-41c7-aa39-515548a39b4a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "/content/drive/MyDrive/Colab Notebooks/intro-to-langsmith-main/notebooks/module_1/utils.py:37: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embd = HuggingFaceEmbeddings(\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2f96b7deaf4de899744acbbe447c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780830b4c0a741cebf86ce203be19dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd2ba51412240deb7064339d45ff875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c77b042796b448c5a2b83962c10622e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9476fafbbe5c4de99a724cd892427426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e065052e2844dfbfe15f69c62acf80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80fbfd698477400a8e08930c04815efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89da5cd3395e491b8326fd89715e4cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a371117173461b86dd3be5448bea1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e743557d8853499dbedcfefc3e039616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6b1ff6c74b4d6eae6ccd6882b57958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing vector store found. Indexing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 197/197 [00:47<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1174 document splits. Creating vector store...\n",
      "Persisting vector store to: /tmp/union_local.parquet\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import operator\n",
    "from langchain.schema import Document\n",
    "from langchain_core.messages import HumanMessage, AnyMessage, get_buffer_string\n",
    "#from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "from utils import get_vector_db_retriever, RAG_PROMPT\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "retriever = get_vector_db_retriever()\n",
    "#llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Define Graph state\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    messages: Annotated[List[AnyMessage], operator.add]\n",
    "    documents: List[Document]\n",
    "\n",
    "# Define Nodes\n",
    "def retrieve_documents(state: GraphState):\n",
    "    messages = state.get(\"messages\", [])\n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.invoke(f\"{get_buffer_string(messages)} {question}\")\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "def generate_response(state: GraphState):\n",
    "    question = state[\"question\"]\n",
    "    messages = state[\"messages\"]\n",
    "    documents = state[\"documents\"]\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "    rag_prompt_formatted = RAG_PROMPT.format(context=formatted_docs, conversation=messages, question=question)\n",
    "    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "    return {\"documents\": documents, \"messages\": [HumanMessage(question), generation]}\n",
    "\n",
    "# Define Graph\n",
    "graph_builder = StateGraph(GraphState)\n",
    "graph_builder.add_node(\"retrieve_documents\", retrieve_documents)\n",
    "graph_builder.add_node(\"generate_response\", generate_response)\n",
    "graph_builder.add_edge(START, \"retrieve_documents\")\n",
    "graph_builder.add_edge(\"retrieve_documents\", \"generate_response\")\n",
    "graph_builder.add_edge(\"generate_response\", END)\n",
    "\n",
    "simple_rag_graph = graph_builder.compile()\n",
    "display(Image(simple_rag_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VBhM1HoBuei"
   },
   "source": [
    "We're setting up a simple graph in LangGraph. If you want to learn more about LangGraph, I would highly recommend taking a look at our LangGraph Academy course.\n",
    "\n",
    "You can also pass in metadata or other fields through an optional config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6579,
     "status": "ok",
     "timestamp": 1756695381329,
     "user": {
      "displayName": "Chaoran Zhou",
      "userId": "09162553986537566448"
     },
     "user_tz": -480
    },
    "id": "x2b7NSB9Buej",
    "outputId": "32c36c60-0dc6-4ddc-c8bd-9329e83e62fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"How do I set up tracing if I'm using LangChain?\",\n",
       " 'messages': [HumanMessage(content=\"How do I set up tracing if I'm using LangChain?\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='If you are using LangChain, you can set up tracing by configuring a few environment variables. You need to set `LANGSMITH_TRACING` to \\'true\\' and provide your `LANGSMITH_API_KEY`. If you are using LangChain modules within LangGraph, you should also set `LANGSMITH_TRACING_BACKGROUND` to \"false\".', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--0c8d31cb-58a4-4f33-9db2-bdff28abd9b1-0', usage_metadata={'input_tokens': 1634, 'output_tokens': 1256, 'total_tokens': 2890, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1183}})],\n",
       " 'documents': [Document(metadata={'id': '73972773-5a16-4c1c-baba-59ed5eac1495', 'source': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Trace with LangGraph - Docs by LangChainOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationSet up tracingIntegrationsLangChainLangGraphAnthropic (Python only)OpenAIInstructorVercel AI SDKOpenTelemetryOpenAI Agents SDKClaude CodeManualConfigurationView tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesSet up webhook notifications for rulesConfigure webhook notificationsSet up online evaluatorsHuman FeedbackLog user feedback using the SDKMonitoringMonitor projects with dashboardsAlertsCommon data typesRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KAsk AIForumForumSearch...NavigationIntegrationsTrace with LangGraphGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationForumOn this pageWith LangChain1. Installation2. Configure your environment3. Log a traceWithout LangChain1. Installation2. Configure your environment3. Log a traceSet up tracingIntegrationsTrace with LangGraphCopy pageCopy pageLangSmith smoothly integrates with LangGraph (Python and JS) to help you trace agentic workflows, whether you’re using LangChain modules or other SDKs.\\n\\u200bWith LangChain\\nIf you are using LangChain modules within LangGraph, you only need to set a few environment variables to enable tracing.\\nThis guide will walk through a basic example. For more detailed information on configuration, see the Trace With LangChain guide.\\n\\u200b1. Installation\\nInstall the LangGraph library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below).\\nFor a full list of packages available, see the LangChain Python docs and LangChain JS docs.\\npipyarnnpmpnpmCopyAsk AIpip install langchain_openai langgraph'),\n",
       "  Document(metadata={'id': '37a142d2-8374-4dde-96f7-0fc5e8f7a983', 'source': 'https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Set an environment variable named LANGSMITH_TRACING_BACKGROUND to \"false\". This will cause your traced functions to wait for tracing to complete before returning.\\n\\nNote that this is named differently from the environment variable in LangChain.js because LangSmith can be used without LangChain.\\n\\n\\nPass a custom client into your traced runs and await the client.awaitPendingTraceBatches(); method.\\n\\nHere’s an example of using awaitPendingTraceBatches alongside the traceable method:\\nCopyAsk AIimport { Client } from \"langsmith\";\\nimport { traceable } from \"langsmith/traceable\";\\nconst langsmithClient = new Client({});\\nconst tracedFn = traceable(\\n  async () => {\\n    return \"Some return value\";\\n  },\\n  {\\n    client: langsmithClient,\\n  }\\n);\\nconst res = await tracedFn();\\nawait langsmithClient.awaitPendingTraceBatches();\\n\\n\\u200bRate limits at high concurrency\\x0b\\nBy default, the LangSmith client will batch operations as your traced run executions, sending a new batch every few milliseconds.\\nThis works well in most situations, but if your traced function is long-running and you have very high concurrency, you may also hit rate limits related to overall request count.\\nIf you are seeing rate limit errors related to this, you can try setting manualFlushMode: true in your client like this:\\nCopyAsk AIimport { Client } from \"langsmith\";\\nconst langsmithClient = new Client({  manualFlushMode: true,});\\nconst myTracedFunc = traceable(\\n  async () => {\\n    // Your logic here...\\n  },\\n  { client: langsmithClient }\\n);\\n\\nAnd then manually calling client.flush() like this before your serverless function closes:\\nCopyAsk AItry {\\n  await myTracedFunc();\\n} finally {\\n  await langsmithClient.flush();\\n}'),\n",
       "  Document(metadata={'id': 'edca3e99-57af-4c07-9f73-4a01b89fa827', 'source': 'https://docs.smith.langchain.com/observability/how_to_guides/annotate_code', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/annotate_code', 'changefreq': 'weekly', 'priority': '0.5'}, page_content=\"There are several ways to log traces to LangSmith.\\nIf you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the LangChain-specific instructions.\\n\\u200bUse @traceable / traceable\\nLangSmith makes it easy to log traces with minimal changes to your existing code with the @traceable decorator in Python and traceable function in TypeScript.\\nThe LANGSMITH_TRACING environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using @traceable or traceable. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGSMITH_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default. To log traces to a different project, see this section.\\nThe @traceable decorator is a simple way to log traces from the LangSmith Python SDK. Simply decorate any function with @traceable.\\nNote that when wrapping a sync function with traceable, (e.g. formatPrompt in the example below), you should use the await keyword when calling it to\\nensure the trace is logged correctly.\\nPythonTypeScriptCopyAsk AIfrom langsmith import traceable\\nfrom openai import Client\"),\n",
       "  Document(metadata={'id': '52c7b7d4-dba0-438a-bd7d-d899ddbc813d', 'source': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='Trace with LangChain (Python and JS/TS) - Docs by LangChainOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationSet up tracingIntegrationsLangChainLangGraphAnthropic (Python only)OpenAIInstructorVercel AI SDKOpenTelemetryOpenAI Agents SDKClaude CodeManualConfigurationView tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesSet up webhook notifications for rulesConfigure webhook notificationsSet up online evaluatorsHuman FeedbackLog user feedback using the SDKMonitoringMonitor projects with dashboardsAlertsCommon data typesRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KAsk AIForumForumSearch...NavigationIntegrationsTrace with LangChain (Python and JS/TS)Get startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationForumOn this pageInstallationQuick start1. Configure your environment2. Log a trace3. View your traceTrace selectivelyLog to a specific projectStaticallyDynamicallyAdd metadata and tags to tracesCustomize run nameCustomize run IDAccess run (span) ID for LangChain invocationsEnsure all traces are submitted before exitingTrace without setting environment variablesDistributed tracing with LangChain (Python)Interoperability between LangChain (Python) and LangSmith SDKInteroperability between LangChain.JS and LangSmith SDKTracing LangChain objects inside traceable (JS only)Tracing LangChain child runs via traceable / RunTree API (JS only)Set up tracingIntegrationsTrace with LangChain (Python and JS/TS)Copy pageCopy pageLangSmith integrates seamlessly with LangChain ( and ), the popular open-source framework for building LLM applications.\\n\\u200bInstallation')]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How do I set up tracing if I'm using LangChain?\"\n",
    "simple_rag_graph.invoke({\"question\": question}, config={\"metadata\": {\"foo\": \"bar\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxTWTIbLBuej"
   },
   "source": [
    "##### Let's take a look in LangSmith!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uROESTMGBuek"
   },
   "source": [
    "## Tracing Context Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ew0xQX9RBuek"
   },
   "source": [
    "In Python, you can use the trace context manager to log traces to LangSmith. This is useful in situations where:\n",
    "\n",
    "You want to log traces for a specific block of code.\n",
    "You want control over the inputs, outputs, and other attributes of the trace.\n",
    "It is not feasible to use a decorator or wrapper.\n",
    "Any or all of the above.\n",
    "The context manager integrates seamlessly with the traceable decorator and wrap_openai wrapper, so you can use them together in the same application.\n",
    "\n",
    "You still need to set your `LANGSMITH_API_KEY` and `LANGSMITH_TRACING`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Q-IJxh_Buek"
   },
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4369,
     "status": "ok",
     "timestamp": 1756695689618,
     "user": {
      "displayName": "Chaoran Zhou",
      "userId": "09162553986537566448"
     },
     "user_tz": -480
    },
    "id": "B079bqnLBuek",
    "outputId": "f5a89217-1a78-4909-eef7-decb17ce6eaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing vector store from: /tmp/union_local.parquet\n"
     ]
    }
   ],
   "source": [
    "from langsmith import traceable, trace\n",
    "# from openai import OpenAI\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "import os\n",
    "from google import genai\n",
    "\n",
    "# MODEL_PROVIDER = \"openai\"\n",
    "# MODEL_NAME = \"gpt-4o-mini\"\n",
    "MODEL_NAME = \"gemini-2.5-flash\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "# openai_client = OpenAI()\n",
    "client = genai.Client(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "\"\"\"\n",
    "retrieve_documents\n",
    "- Returns documents fetched from a vectorstore based on the user's question\n",
    "\"\"\"\n",
    "@traceable\n",
    "def retrieve_documents(question: str):\n",
    "    documents = retriever.invoke(question)\n",
    "    return documents\n",
    "\n",
    "\"\"\"\n",
    "generate_response\n",
    "- Calls `call_gemini` to generate a model response after formatting inputs\n",
    "\"\"\"\n",
    "# TODO: Remove traceable, and use with trace()\n",
    "\n",
    "def generate_response(question: str, documents):\n",
    "    # NOTE: Our documents came in as a list of objects, but we just want to log a string\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "    # TODO: Use with trace()\n",
    "    with trace(\n",
    "        name=\"Generate Response\",\n",
    "        run_type=\"chain\",\n",
    "        inputs={\"question\": question, \"formatted_docs\": formatted_docs},\n",
    "        metadata={\"foo\": \"bar\"},\n",
    "    ) as ls_trace:\n",
    "    # messages = [\n",
    "    #     {\n",
    "    #         \"role\": \"system\",\n",
    "    #         \"content\": RAG_SYSTEM_PROMPT\n",
    "    #     },\n",
    "    #     {\n",
    "    #         \"role\": \"user\",\n",
    "    #         \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "    #     }\n",
    "    # ]\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"parts\": [\n",
    "                    {\"text\": RAG_SYSTEM_PROMPT},\n",
    "                    {\"text\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        response = call_gemini(messages)\n",
    "        # TODO: End your trace and write outputs to LangSmith\n",
    "        ls_trace.end(outputs={\"output\": response})\n",
    "    return response\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "call_gemini\n",
    "- Returns the chat completion output from OpenAI\n",
    "\"\"\"\n",
    "@traceable\n",
    "def call_gemini(\n",
    "    messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0\n",
    ") -> str:\n",
    "    # response = openai_client.chat.completions.create(\n",
    "    #     model=model,\n",
    "    #     messages=messages,\n",
    "    #     temperature=temperature,\n",
    "    # )\n",
    "    # return response\n",
    "    return client.models.generate_content(\n",
    "    model=model, contents=messages\n",
    "    )\n",
    "\n",
    "\"\"\"\n",
    "langsmith_rag\n",
    "- Calls `retrieve_documents` to fetch documents\n",
    "- Calls `generate_response` to generate a response based on the fetched documents\n",
    "- Returns the model response\n",
    "\"\"\"\n",
    "@traceable\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    # return response.choices[0].message.content\n",
    "    # Access the text content from the response\n",
    "    return response.candidates[0].content.parts[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3303,
     "status": "ok",
     "timestamp": 1756695695390,
     "user": {
      "displayName": "Chaoran Zhou",
      "userId": "09162553986537566448"
     },
     "user_tz": -480
    },
    "id": "SgbU5pvBBuel",
    "outputId": "62e097cc-3d60-48bf-e84b-83297f555c02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Python, you can trace selectively by using the `ls.tracing_context` context manager from the `langsmith` library. To enable tracing for specific invocations, wrap your code within `with ls.tracing_context(enabled=True):`. You can also disable tracing for a particular block using `with ls.tracing_context(enabled=False):`, even if `LANGSMITH_TRACING` is set to true.\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I trace with tracing context?\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pJiKym-Buem"
   },
   "source": [
    "## wrap_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRQ838RmBuem"
   },
   "source": [
    "The wrap_openai/wrapOpenAI methods in Python/TypeScript allow you to wrap your OpenAI client in order to automatically log traces -- no decorator or function wrapping required! The wrapper works seamlessly with the @traceable decorator or traceable function and you can use both in the same application.\n",
    "\n",
    "You still need to set your `LANGSMITH_API_KEY` and `LANGSMITH_TRACING`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWkFCqmmBuem"
   },
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3762,
     "status": "ok",
     "timestamp": 1756696071704,
     "user": {
      "displayName": "Chaoran Zhou",
      "userId": "09162553986537566448"
     },
     "user_tz": -480
    },
    "id": "uY_cwsM5Buem",
    "outputId": "df1d3ebb-16db-4348-fcc4-c15e731ace3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing vector store from: /tmp/union_local.parquet\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import wrap_openai\n",
    "from langsmith.wrappers import wrap_openai\n",
    "import openai\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "import os\n",
    "from google import genai\n",
    "\n",
    "# MODEL_PROVIDER = \"openai\"\n",
    "# MODEL_NAME = \"gpt-4o-mini\"\n",
    "MODEL_NAME = \"gemini-2.5-flash\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Wrap the OpenAI Client\n",
    "# openai_client = OpenAI()\n",
    "client = wrap_openai(genai.Client(api_key=os.getenv('GOOGLE_API_KEY')))\n",
    "\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    # messages = [\n",
    "    #     {\n",
    "    #         \"role\": \"system\",\n",
    "    #         \"content\": RAG_SYSTEM_PROMPT\n",
    "    #     },\n",
    "    #     {\n",
    "    #         \"role\": \"user\",\n",
    "    #         \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "    #     }\n",
    "    # ]\n",
    "    messages = [\n",
    "                  {\n",
    "                      \"role\": \"user\",\n",
    "                      \"parts\": [\n",
    "                          {\"text\": RAG_SYSTEM_PROMPT},\n",
    "                          {\"text\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"}\n",
    "                      ]\n",
    "                  }\n",
    "              ]\n",
    "    # TODO: We don't need to use @traceable on a nested function call anymore,\n",
    "    # wrap_openai takes care of this for us\n",
    "    return call_gemini(messages)\n",
    "\n",
    "@traceable\n",
    "def call_gemini(\n",
    "    messages: List[dict],\n",
    ") -> str:\n",
    "    # return openai_client.chat.completions.create(\n",
    "    #     model=MODEL_NAME,\n",
    "    #     messages=messages,\n",
    "    # )\n",
    "    return client.models.generate_content(\n",
    "    model=MODEL_NAME, contents=messages\n",
    "    )\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag_with_wrap_openai(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    # return response.choices[0].message.content\n",
    "    # Access the text content from the response\n",
    "    return response.candidates[0].content.parts[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2729,
     "status": "ok",
     "timestamp": 1756695911001,
     "user": {
      "displayName": "Chaoran Zhou",
      "userId": "09162553986537566448"
     },
     "user_tz": -480
    },
    "id": "TXxLboLaBuen",
    "outputId": "324cbbc3-3aee-4f67-ecc0-d5c38bd755ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To trace with `wrap_openai`, first import `wrappers` from `langsmith` and `OpenAI` from `openai`. Then, create an instance of the `OpenAI` client and wrap it using `oai_client = wrappers.wrap_openai(OpenAI())`. Ensure the `LANGSMITH_TRACING` environment variable is set for the wrapper to function correctly, and all subsequent model calls made with `oai_client` will be automatically traced.\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I trace with wrap_openai?\"\n",
    "ai_answer = langsmith_rag_with_wrap_openai(question)\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2I_br7xMBuen"
   },
   "source": [
    "The wrapped OpenAI client accepts all the same langsmith_extra parameters as @traceable decorated functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wpa1_XI2Buep"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What color is the sky?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "openai_client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=messages,\n",
    "    langsmith_extra={\"metadata\": {\"foo\": \"bar\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpndV_gABuep",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## [Advanced] RunTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJK3WGhHBueq"
   },
   "source": [
    "Another, more explicit way to log traces to LangSmith is via the RunTree API. This API allows you more control over your tracing - you can manually create runs and children runs to assemble your trace. You still need to set your `LANGSMITH_API_KEY`, but `LANGSMITH_TRACING` is not necessary for this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLyILIKfBueq"
   },
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BAH2sDIWBueq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AtVqjRYWBueq"
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "# I have my env variables defined in a .env file\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXp2klRtBueq"
   },
   "source": [
    "Let's go ahead and set `LANGSMITH_TRACING` to false, as we are using RunTree to manually create runs in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1756612880517,
     "user": {
      "displayName": "Chaoran Zhou",
      "userId": "09162553986537566448"
     },
     "user_tz": -480
    },
    "id": "qZOLuYVhBuer",
    "outputId": "eca74093-7cf9-43a4-d523-5b625e7595c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"false\"\n",
    "\n",
    "from langsmith import utils\n",
    "utils.tracing_is_enabled() # This should return false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bkDZFvgBuer"
   },
   "source": [
    "We have rewritten our RAG application, except this time we pass a RunTree argument through our function calls, and create child runs at each layer. This gives our RunTree the same hierarchy that we were automatically able to establish with @traceable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4929,
     "status": "ok",
     "timestamp": 1756613134474,
     "user": {
      "displayName": "Chaoran Zhou",
      "userId": "09162553986537566448"
     },
     "user_tz": -480
    },
    "id": "oClxdIxeBuer",
    "outputId": "a4dd646c-b114-4c62-de79-5820ac851867"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing vector store from: /tmp/union_local.parquet\n"
     ]
    }
   ],
   "source": [
    "from langsmith import RunTree\n",
    "# from openai import OpenAI\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "# openai_client = OpenAI()\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "def retrieve_documents(parent_run: RunTree, question: str):\n",
    "    # Create a child run\n",
    "    child_run = parent_run.create_child(\n",
    "        name=\"Retrieve Documents\",\n",
    "        run_type=\"retriever\",\n",
    "        inputs={\"question\": question},\n",
    "    )\n",
    "    documents = retriever.invoke(question)\n",
    "    # Post the output of our child run\n",
    "    child_run.end(outputs={\"documents\": documents})\n",
    "    child_run.post()\n",
    "    return documents\n",
    "\n",
    "def generate_response(parent_run: RunTree, question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    rag_system_prompt = \"\"\"You are an assistant for question-answering tasks.\n",
    "    Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
    "    If you don't know the answer, just say that you don't know.\n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    \"\"\"\n",
    "    # Create a child run\n",
    "    child_run = parent_run.create_child(\n",
    "        name=\"Generate Response\",\n",
    "        run_type=\"chain\",\n",
    "        inputs={\"question\": question, \"documents\": documents},\n",
    "    )\n",
    "    # messages = [\n",
    "    #     {\n",
    "    #         \"role\": \"system\",\n",
    "    #         \"content\": rag_system_prompt\n",
    "    #     },\n",
    "    #     {\n",
    "    #         \"role\": \"user\",\n",
    "    #         \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "    #     }\n",
    "    # ]\n",
    "    messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"parts\": [\n",
    "                        {\"text\": RAG_SYSTEM_PROMPT},\n",
    "                        {\"text\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "    gemini_response = call_gemini(child_run, messages)\n",
    "    # Post the output of our child run\n",
    "    child_run.end(outputs={\"gemini_response\": gemini_response})\n",
    "    child_run.post()\n",
    "    return gemini_response\n",
    "\n",
    "def call_gemini(\n",
    "    parent_run: RunTree, messages: List[dict], model: str = \"gpt-4o-mini\", temperature: float = 0.0\n",
    ") -> str:\n",
    "    # Create a child run\n",
    "    child_run = parent_run.create_child(\n",
    "        name=\"OpenAI Call\",\n",
    "        run_type=\"llm\",\n",
    "        inputs={\"messages\": messages},\n",
    "    )\n",
    "    # openai_response = openai_client.chat.completions.create(\n",
    "    #     model=model,\n",
    "    #     messages=messages,\n",
    "    #     temperature=temperature,\n",
    "    # )\n",
    "    gemini_response = client.models.generate_content(\n",
    "    model=MODEL_NAME, contents=messages\n",
    "    )\n",
    "    # Post the output of our child run\n",
    "    child_run.end(outputs={\"gemini_response\": gemini_response})\n",
    "    child_run.post()\n",
    "    return gemini_response\n",
    "\n",
    "def langsmith_rag(question: str):\n",
    "    # Create a root RunTree\n",
    "    root_run_tree = RunTree(\n",
    "        name=\"Chat Pipeline\",\n",
    "        run_type=\"chain\",\n",
    "        inputs={\"question\": question}\n",
    "    )\n",
    "\n",
    "    # Pass our RunTree into the nested function calls\n",
    "    documents = retrieve_documents(root_run_tree, question)\n",
    "    response = generate_response(root_run_tree, question, documents)\n",
    "    # output = response.choices[0].message.content\n",
    "    output = response.candidates[0].content.parts[0].text\n",
    "\n",
    "    # Post our final output\n",
    "    root_run_tree.end(outputs={\"generation\": output})\n",
    "    root_run_tree.post()\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2807,
     "status": "ok",
     "timestamp": 1756613553280,
     "user": {
      "displayName": "Chaoran Zhou",
      "userId": "09162553986537566448"
     },
     "user_tz": -480
    },
    "id": "9Npx4zfVBuer",
    "outputId": "a0a6465f-9132-4222-fc1b-3fac357100f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can trace with `RunTree` by converting a `RunnableConfig` to a `RunTree` object using `RunTree.fromRunnableConfig`. Alternatively, you can pass the `RunnableConfig` as the first argument to a `traceable`-wrapped function. Inside a traceable function, you can access the current run's `RunTree` object using `get_current_run_tree()`.\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I trace with RunTree?\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(ai_answer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
