{"cells":[{"cell_type":"markdown","metadata":{"id":"mFpnNvv59eCe"},"source":["# Evaluators"]},{"cell_type":"markdown","metadata":{"id":"WvDmQpZO9eCg"},"source":["At a high-level, an evaluator judges an invocation of your LLM application against a reference example, and returns an evaluation score.\n","\n","In LangSmith evaluators, we represent this process as a function that takes in a Run (representing the LLM app invocation) and an Example (representing the data point to evaluate), and returns Feedback (representing the evaluator's score of the LLM app invocation)."]},{"cell_type":"markdown","metadata":{"id":"lU4Tbj6a9eCi"},"source":["![Evaluator](../../images/evaluator.png)"]},{"cell_type":"markdown","metadata":{"id":"ea_26V3X9eCl"},"source":["Here is an example of a very simple custom evaluator that compares the output of a model to the expected output in the dataset:"]},{"cell_type":"code","source":["### Mount Notebook to Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","# change the working directory to the Drive root\n","%cd /content/drive/My\\ Drive/Colab\\ Notebooks/intro-to-langsmith-main/notebooks/module_2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MrG5vvjFGgOG","executionInfo":{"status":"ok","timestamp":1756706696322,"user_tz":-480,"elapsed":42377,"user":{"displayName":"Chaoran Zhou","userId":"09162553986537566448"}},"outputId":"ae07dbf4-0c15-4a82-d844-faf8f7be3fed"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/Colab Notebooks/intro-to-langsmith-main/notebooks/module_2\n"]}]},{"cell_type":"code","source":["!pip install --quiet -U langchain-google-genai langgraph langgraph-sdk langgraph-checkpoint-sqlite langsmith langchain-community langchain-core\n","!pip install --quiet notebook python-dotenv lxml scikit-learn pandas pyarrow"],"metadata":{"id":"pOIAnY-oGxnX","executionInfo":{"status":"ok","timestamp":1756706735435,"user_tz":-480,"elapsed":35905,"user":{"displayName":"Chaoran Zhou","userId":"09162553986537566448"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"41fceef8-c91d-48f6-9283-4f7dee8f1497"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.5/378.5 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.0/444.0 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.6/151.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n","google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["!pip install --quiet python-dotenv"],"metadata":{"id":"n9k9ZKUaHA5l","executionInfo":{"status":"ok","timestamp":1756706756522,"user_tz":-480,"elapsed":15478,"user":{"displayName":"Chaoran Zhou","userId":"09162553986537566448"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from dotenv import load_dotenv\n","load_dotenv(\".env\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nB4-CZXlHAbB","executionInfo":{"status":"ok","timestamp":1756706760247,"user_tz":-480,"elapsed":353,"user":{"displayName":"Chaoran Zhou","userId":"09162553986537566448"}},"outputId":"59771b11-0ee3-4f54-e3b6-bbdcab4b18cb"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","execution_count":5,"metadata":{"id":"v0XVyLSd9eCm","executionInfo":{"status":"ok","timestamp":1756706803678,"user_tz":-480,"elapsed":641,"user":{"displayName":"Chaoran Zhou","userId":"09162553986537566448"}}},"outputs":[],"source":["from langsmith.schemas import Example, Run\n","\n","def correct_label(inputs: dict, reference_outputs: dict, outputs: dict) -> dict:\n","  score = outputs.get(\"output\") == reference_outputs.get(\"label\")\n","  return {\"score\": int(score), \"key\": \"correct_label\"}"]},{"cell_type":"markdown","metadata":{"id":"XQ1x_cfT9eCn"},"source":["### LLM-as-Judge Evaluation"]},{"cell_type":"markdown","metadata":{"id":"IarXd4ID9eCo"},"source":["LLM-as-judge evaluators use LLMs to score system output. To use them, you typically encode the grading rules / criteria in the LLM prompt. They can be reference-free (e.g., check if system output contains offensive content or adheres to specific criteria). Or, they can compare task output to a reference (e.g., check if the output is factually accurate relative to the reference).\n","\n","Here is an example of how you might define an LLM-as-judge evaluator with structured output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zzvq8VEK9eCq"},"outputs":[],"source":["# You can set them inline\n","import os\n","os.environ[\"OPENAI_API_KEY\"] = \"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0VzL_Roh9eCr"},"outputs":[],"source":["# Or you can use a .env file\n","from dotenv import load_dotenv\n","load_dotenv(dotenv_path=\"../../.env\", override=True)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"ZEVUgQdW9eCs","executionInfo":{"status":"ok","timestamp":1756706827611,"user_tz":-480,"elapsed":3952,"user":{"displayName":"Chaoran Zhou","userId":"09162553986537566448"}}},"outputs":[],"source":["# from openai import OpenAI\n","from pydantic import BaseModel, Field\n","import json\n","import os\n","from google import genai\n","\n","# openai_client = OpenAI()\n","client = genai.Client(api_key=os.getenv('GOOGLE_API_KEY'))\n","\n","class Similarity_Score(BaseModel):\n","    similarity_score: int = Field(description=\"Semantic similarity score between 1 and 10, where 1 means unrelated and 10 means identical.\")\n","\n","# NOTE: This is our evaluator\n","def compare_semantic_similarity(inputs: dict, reference_outputs: dict, outputs: dict):\n","    input_question = inputs[\"question\"]\n","    reference_response = reference_outputs[\"output\"]\n","    run_response = outputs[\"output\"]\n","\n","    # completion = client.beta.chat.completions.parse(\n","    #     model=\"gpt-4o\",\n","    #     messages=[\n","    #         {\n","    #             \"role\": \"system\",\n","    #             \"content\": (\n","    #                 \"You are a semantic similarity evaluator. Compare the meanings of two responses to a question, \"\n","    #                 \"Reference Response and New Response, where the reference is the correct answer, and we are trying to judge if the new response is similar. \"\n","    #                 \"Provide a score between 1 and 10, where 1 means completely unrelated, and 10 means identical in meaning.\"\n","    #             ),\n","    #         },\n","    #         {\"role\": \"user\", \"content\": f\"Question: {input_question}\\n Reference Response: {reference_response}\\n Run Response: {run_response}\"}\n","    #     ],\n","    #     response_format=Similarity_Score,\n","    # )\n","\n","    completion = client.models.generate_content(\n","    model=\"gemini-2.5-flash\", contents=[\n","             {\n","                      \"role\": \"user\",\n","                      \"parts\": [\n","                          {\"text\": \"\"\"You are a semantic similarity evaluator. Compare the meanings of two responses to a question,\n","                    Reference Response and New Response, where the reference is the correct answer, and we are trying to judge if the new response is similar.\n","                    Provide a score between 1 and 10, where 1 means completely unrelated, and 10 means identical in meaning.\"\"\"},\n","                          {\"text\":  f\"Question: {input_question}\\n Reference Response: {reference_response}\\n Run Response: {run_response}\"}\n","                      ]\n","              }],\n","             config={\n","        \"response_mime_type\": \"application/json\",\n","        \"response_schema\": Similarity_Score,\n","    },\n","    )\n","    # print(completion.candidates[0].content.parts[0].text )\n","    # similarity_score = completion.candidates[0].content.parts[0].text # completion.choices[0].message.parsed\n","    # Your JSON string from the API response\n","    json_string = completion.candidates[0].content.parts[0].text\n","\n","    # Use json.loads() to parse the string into a Python dictionary\n","    data_dictionary = json.loads(json_string)\n","\n","    # Now access the value using its key\n","    similarity_score = data_dictionary[\"similarity_score\"]\n","\n","    # print(similarity_score) # This will also print: 1\n","    return {\"score\": similarity_score, \"key\": \"similarity\"}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5JpZc_mP9eCs"},"source":["Let's try this out!\n","\n","NOTE: We purposely made this answer wrong, so we expect to see a low score."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VQwLrzWd9eCs","executionInfo":{"status":"ok","timestamp":1756706842421,"user_tz":-480,"elapsed":4400,"user":{"displayName":"Chaoran Zhou","userId":"09162553986537566448"}},"outputId":"b29dd600-9d35-435a-8900-110d50f96ff0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Semantic similarity score: {'score': 1, 'key': 'similarity'}\n"]}],"source":["# From Dataset Example\n","inputs = {\n","  \"question\": \"Is LangSmith natively integrated with LangChain?\"\n","}\n","reference_outputs = {\n","  \"output\": \"Yes, LangSmith is natively integrated with LangChain, as well as LangGraph.\"\n","}\n","\n","\n","# From Run\n","outputs = {\n","  \"output\": \"No, LangSmith is NOT integrated with LangChain.\"\n","}\n","\n","similarity_score = compare_semantic_similarity(inputs, reference_outputs, outputs)\n","print(f\"Semantic similarity score: {similarity_score}\")"]},{"cell_type":"markdown","metadata":{"id":"zLxzPKZ_9eCt"},"source":["You can also define evaluators using Run and Example directly!"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"vxPu3Lbk9eCt","executionInfo":{"status":"ok","timestamp":1756706942816,"user_tz":-480,"elapsed":3,"user":{"displayName":"Chaoran Zhou","userId":"09162553986537566448"}}},"outputs":[],"source":["from langsmith.schemas import Run, Example\n","\n","def compare_semantic_similarity_v2(root_run: Run, example: Example):\n","    input_question = example[\"inputs\"][\"question\"]\n","    reference_response = example[\"outputs\"][\"output\"]\n","    run_response = root_run[\"outputs\"][\"output\"]\n","\n","    # completion = client.beta.chat.completions.parse(\n","    #     model=\"gpt-4o\",\n","    #     messages=[\n","    #         {\n","    #             \"role\": \"system\",\n","    #             \"content\": (\n","    #                 \"You are a semantic similarity evaluator. Compare the meanings of two responses to a question, \"\n","    #                 \"Reference Response and New Response, where the reference is the correct answer, and we are trying to judge if the new response is similar. \"\n","    #                 \"Provide a score between 1 and 10, where 1 means completely unrelated, and 10 means identical in meaning.\"\n","    #             ),\n","    #         },\n","    #         {\"role\": \"user\", \"content\": f\"Question: {input_question}\\n Reference Response: {reference_response}\\n Run Response: {run_response}\"}\n","    #     ],\n","    #     response_format=Similarity_Score,\n","    # )\n","\n","    # similarity_score = completion.choices[0].message.parsed\n","    # return {\"score\": similarity_score.similarity_score, \"key\": \"similarity\"}\n","\n","    completion = client.models.generate_content(\n","    model=\"gemini-2.5-flash\", contents=[\n","             {\n","                      \"role\": \"user\",\n","                      \"parts\": [\n","                          {\"text\": \"\"\"You are a semantic similarity evaluator. Compare the meanings of two responses to a question,\n","                    Reference Response and New Response, where the reference is the correct answer, and we are trying to judge if the new response is similar.\n","                    Provide a score between 1 and 10, where 1 means completely unrelated, and 10 means identical in meaning.\"\"\"},\n","                          {\"text\":  f\"Question: {input_question}\\n Reference Response: {reference_response}\\n Run Response: {run_response}\"}\n","                      ]\n","              }],\n","             config={\n","        \"response_mime_type\": \"application/json\",\n","        \"response_schema\": Similarity_Score,\n","    },\n","    )\n","    # print(completion.candidates[0].content.parts[0].text )\n","    # similarity_score = completion.candidates[0].content.parts[0].text # completion.choices[0].message.parsed\n","    # Your JSON string from the API response\n","    json_string = completion.candidates[0].content.parts[0].text\n","\n","    # Use json.loads() to parse the string into a Python dictionary\n","    data_dictionary = json.loads(json_string)\n","\n","    # Now access the value using its key\n","    similarity_score = data_dictionary[\"similarity_score\"]\n","\n","    # print(similarity_score) # This will also print: 1\n","    return {\"score\": similarity_score, \"key\": \"similarity\"}\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pPaAtHL39eCt","executionInfo":{"status":"ok","timestamp":1756706947674,"user_tz":-480,"elapsed":1263,"user":{"displayName":"Chaoran Zhou","userId":"09162553986537566448"}},"outputId":"73b3db4d-151b-4e03-e807-b7379ff4be0c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Semantic similarity score: {'score': 1, 'key': 'similarity'}\n"]}],"source":["sample_run = {\n","  \"name\": \"Sample Run\",\n","  \"inputs\": {\n","    \"question\": \"Is LangSmith natively integrated with LangChain?\"\n","  },\n","  \"outputs\": {\n","    \"output\": \"No, LangSmith is NOT integrated with LangChain.\"\n","  },\n","  \"is_root\": True,\n","  \"status\": \"success\",\n","  \"extra\": {\n","    \"metadata\": {\n","      \"key\": \"value\"\n","    }\n","  }\n","}\n","\n","sample_example = {\n","  \"inputs\": {\n","    \"question\": \"Is LangSmith natively integrated with LangChain?\"\n","  },\n","  \"outputs\": {\n","    \"output\": \"Yes, LangSmith is natively integrated with LangChain, as well as LangGraph.\"\n","  },\n","  \"metadata\": {\n","    \"dataset_split\": [\n","      \"AI generated\",\n","      \"base\"\n","    ]\n","  }\n","}\n","\n","similarity_score = compare_semantic_similarity_v2(sample_run, sample_example)\n","print(f\"Semantic similarity score: {similarity_score}\")"]}],"metadata":{"kernelspec":{"display_name":"ls-academy","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}