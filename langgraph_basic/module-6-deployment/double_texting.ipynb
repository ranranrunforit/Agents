{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1FKJsk6cFdY"
      },
      "source": [
        "# Double Texting\n",
        "\n",
        "Seamless handling of [double texting](https://langchain-ai.github.io/langgraph/concepts/double_texting/) is important for handling real-world usage scenarios, especially in chat applications.\n",
        "\n",
        "Users can send multiple messages in a row before the prior run(s) complete, and we want to ensure that we handle this gracefully.\n",
        "\n",
        "## Reject\n",
        "\n",
        "A simple approach is to [reject](https://langchain-ai.github.io/langgraph/cloud/how-tos/reject_concurrent/) any new runs until the current run completes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Mount Notebook to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# change the working directory to the Drive root\n",
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/langchain-academy-main/module-6"
      ],
      "metadata": {
        "id": "F7cfi0WHcIm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet -U langgraph_sdk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOIAnY-oGxnX",
        "outputId": "530e87cb-1f02-45ca-d474-a5dc4a9c6e84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.0/444.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet python-dotenv"
      ],
      "metadata": {
        "id": "n9k9ZKUaHA5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv(\".env\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB4-CZXlHAbB",
        "outputId": "45ce9562-480f-44f0-f6a8-d5a37860d4c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uSshlEgcFde"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U langgraph_sdk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRtZY1TjcFdg"
      },
      "outputs": [],
      "source": [
        "from langgraph_sdk import get_client\n",
        "url_for_cli_deployment = \"http://localhost:8123\"\n",
        "client = get_client(url=url_for_cli_deployment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNjBM7uGcFdg",
        "outputId": "a3a8f6a7-76f0-411e-c7a9-9a807158bb85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to start concurrent run Client error '409 Conflict' for url 'http://localhost:8123/threads/2b58630e-00fd-4c35-afad-a6b59e9b9104/runs'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/409\n"
          ]
        }
      ],
      "source": [
        "import httpx\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# Create a thread\n",
        "thread = await client.threads.create()\n",
        "\n",
        "# Create to dos\n",
        "user_input_1 = \"Add a ToDo to follow-up with DI Repairs.\"\n",
        "user_input_2 = \"Add a ToDo to mount dresser to the wall.\"\n",
        "config = {\"configurable\": {\"user_id\": \"Test-Double-Texting\"}}\n",
        "graph_name = \"task_maistro\"\n",
        "\n",
        "run = await client.runs.create(\n",
        "    thread[\"thread_id\"],\n",
        "    graph_name,\n",
        "    input={\"messages\": [HumanMessage(content=user_input_1)]},\n",
        "    config=config,\n",
        ")\n",
        "try:\n",
        "    await client.runs.create(\n",
        "        thread[\"thread_id\"],\n",
        "        graph_name,\n",
        "        input={\"messages\": [HumanMessage(content=user_input_2)]},\n",
        "        config=config,\n",
        "        multitask_strategy=\"reject\",\n",
        "    )\n",
        "except httpx.HTTPStatusError as e:\n",
        "    print(\"Failed to start concurrent run\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TzbZ--bcFdi",
        "outputId": "e3d877d0-157b-4bf9-f06f-3df20c2a915e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Add a ToDo to follow-up with DI Repairs.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  UpdateMemory (call_6xqHubCPNufS0bg4tbUxC0FU)\n",
            " Call ID: call_6xqHubCPNufS0bg4tbUxC0FU\n",
            "  Args:\n",
            "    update_type: todo\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "\n",
            "New ToDo created:\n",
            "Content: {'task': 'Follow-up with DI Repairs', 'time_to_complete': 30, 'deadline': None, 'solutions': ['Call DI Repairs customer service', 'Email DI Repairs support', 'Check DI Repairs website for updates'], 'status': 'not started'}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I've added a task to follow-up with DI Repairs to your ToDo list. If there's anything else you need, feel free to let me know!\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import convert_to_messages\n",
        "\n",
        "# Wait until the original run completes\n",
        "await client.runs.join(thread[\"thread_id\"], run[\"run_id\"])\n",
        "\n",
        "# Get the state of the thread\n",
        "state = await client.threads.get_state(thread[\"thread_id\"])\n",
        "for m in convert_to_messages(state[\"values\"][\"messages\"]):\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMGUnc94cFdj"
      },
      "source": [
        "## Enqueue\n",
        "\n",
        "We can use [enqueue](https://langchain-ai.github.io/langgraph/cloud/how-tos/enqueue_concurrent/https://langchain-ai.github.io/langgraph/cloud/how-tos/enqueue_concurrent/) any new runs until the current run completes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXd31ucZcFdj",
        "outputId": "6b302d75-5bb4-4fd1-8797-ae0414cc01e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Send Erik his t-shirt gift this weekend.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  UpdateMemory (call_svTeXPmWGTLY8aQ8EifjwHAa)\n",
            " Call ID: call_svTeXPmWGTLY8aQ8EifjwHAa\n",
            "  Args:\n",
            "    update_type: todo\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "\n",
            "New ToDo created:\n",
            "Content: {'task': 'Send Erik his t-shirt gift', 'time_to_complete': 30, 'deadline': '2024-11-19T23:59:00', 'solutions': ['Wrap the t-shirt', \"Get Erik's address\", 'Visit the post office', 'Choose a delivery service'], 'status': 'not started'}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I've updated your ToDo list to send Erik his t-shirt gift this weekend. If there's anything else you need, feel free to let me know!\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Get cash and pay nanny for 2 weeks. Do this by Friday.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  UpdateMemory (call_Cq0Tfn6yqccHH8n0DOucz5OQ)\n",
            " Call ID: call_Cq0Tfn6yqccHH8n0DOucz5OQ\n",
            "  Args:\n",
            "    update_type: todo\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "\n",
            "New ToDo created:\n",
            "Content: {'task': 'Get cash and pay nanny for 2 weeks', 'time_to_complete': 15, 'deadline': '2024-11-17T23:59:00', 'solutions': ['Visit the ATM', 'Calculate the total amount for 2 weeks', 'Hand over the cash to the nanny'], 'status': 'not started'}\n",
            "\n",
            "Document af1fe011-f3c5-4c1c-b98b-181869bc2944 updated:\n",
            "Plan: Update the deadline for sending Erik his t-shirt gift to this weekend, which is by 2024-11-17.\n",
            "Added content: 2024-11-17T23:59:00\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I've updated your ToDo list to ensure you get cash and pay the nanny for 2 weeks by Friday. Let me know if there's anything else you need!\n"
          ]
        }
      ],
      "source": [
        "# Create a new thread\n",
        "thread = await client.threads.create()\n",
        "\n",
        "# Create new ToDos\n",
        "user_input_1 = \"Send Erik his t-shirt gift this weekend.\"\n",
        "user_input_2 = \"Get cash and pay nanny for 2 weeks. Do this by Friday.\"\n",
        "config = {\"configurable\": {\"user_id\": \"Test-Double-Texting\"}}\n",
        "graph_name = \"task_maistro\"\n",
        "\n",
        "first_run = await client.runs.create(\n",
        "    thread[\"thread_id\"],\n",
        "    graph_name,\n",
        "    input={\"messages\": [HumanMessage(content=user_input_1)]},\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "second_run = await client.runs.create(\n",
        "    thread[\"thread_id\"],\n",
        "    graph_name,\n",
        "    input={\"messages\": [HumanMessage(content=user_input_2)]},\n",
        "    config=config,\n",
        "    multitask_strategy=\"enqueue\",\n",
        ")\n",
        "\n",
        "# Wait until the second run completes\n",
        "await client.runs.join(thread[\"thread_id\"], second_run[\"run_id\"])\n",
        "\n",
        "# Get the state of the thread\n",
        "state = await client.threads.get_state(thread[\"thread_id\"])\n",
        "for m in convert_to_messages(state[\"values\"][\"messages\"]):\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIrAZUJDcFdk"
      },
      "source": [
        "## Interrupt\n",
        "\n",
        "We can use [interrupt](https://langchain-ai.github.io/langgraph/cloud/how-tos/interrupt_concurrent/) to interrupt the current run, but save all the work that has been done so far up to that point.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uacSIzmucFdk",
        "outputId": "23475078-c2e9-4193-98d9-2063663f740d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Give me a summary of my ToDos due tomrrow.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Never mind, create a ToDo to Order Ham for Thanksgiving by next Friday.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  UpdateMemory (call_Rk80tTSJzik2oY44tyUWk8FM)\n",
            " Call ID: call_Rk80tTSJzik2oY44tyUWk8FM\n",
            "  Args:\n",
            "    update_type: todo\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "\n",
            "New ToDo created:\n",
            "Content: {'task': 'Order Ham for Thanksgiving', 'time_to_complete': 30, 'deadline': '2024-11-22T23:59:59', 'solutions': ['Check local grocery stores for availability', 'Order online from a specialty meat provider', 'Visit a local butcher shop'], 'status': 'not started'}\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I've added the task \"Order Ham for Thanksgiving\" to your ToDo list with a deadline of next Friday. If you need any more help, feel free to ask!\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "\n",
        "# Create a new thread\n",
        "thread = await client.threads.create()\n",
        "\n",
        "# Create new ToDos\n",
        "user_input_1 = \"Give me a summary of my ToDos due tomrrow.\"\n",
        "user_input_2 = \"Never mind, create a ToDo to Order Ham for Thanksgiving by next Friday.\"\n",
        "config = {\"configurable\": {\"user_id\": \"Test-Double-Texting\"}}\n",
        "graph_name = \"task_maistro\"\n",
        "\n",
        "interrupted_run = await client.runs.create(\n",
        "    thread[\"thread_id\"],\n",
        "    graph_name,\n",
        "    input={\"messages\": [HumanMessage(content=user_input_1)]},\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "# Wait for some of run 1 to complete so that we can see it in the thread\n",
        "await asyncio.sleep(1)\n",
        "\n",
        "second_run = await client.runs.create(\n",
        "    thread[\"thread_id\"],\n",
        "    graph_name,\n",
        "    input={\"messages\": [HumanMessage(content=user_input_2)]},\n",
        "    config=config,\n",
        "    multitask_strategy=\"interrupt\",\n",
        ")\n",
        "\n",
        "# Wait until the second run completes\n",
        "await client.runs.join(thread[\"thread_id\"], second_run[\"run_id\"])\n",
        "\n",
        "# Get the state of the thread\n",
        "state = await client.threads.get_state(thread[\"thread_id\"])\n",
        "for m in convert_to_messages(state[\"values\"][\"messages\"]):\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKAUg5tCcFdl"
      },
      "source": [
        "We can see the initial run is saved, and has status `interrupted`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtAqWgz5cFdl",
        "outputId": "5dfff919-caf1-4047-bf20-69e363a9e47d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "interrupted\n"
          ]
        }
      ],
      "source": [
        "# Confirm that the first run was interrupted\n",
        "print((await client.runs.get(thread[\"thread_id\"], interrupted_run[\"run_id\"]))[\"status\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpTsku2zcFdm"
      },
      "source": [
        "## Rollback\n",
        "\n",
        "We can use [rollback](https://langchain-ai.github.io/langgraph/cloud/how-tos/rollback_concurrent/) to interrupt the prior run of the graph, delete it, and start a new run with the double-texted input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqXxU-bgcFdn",
        "outputId": "a4c7c709-b3e4-4612-9b10-d0b0309221de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Actually, add a ToDo to drop by Yoga in person on Sunday.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "It looks like the task \"Drop by Yoga in person\" is already on your ToDo list with a deadline of November 19, 2024. Would you like me to update the deadline to the upcoming Sunday instead?\n"
          ]
        }
      ],
      "source": [
        "# Create a new thread\n",
        "thread = await client.threads.create()\n",
        "\n",
        "# Create new ToDos\n",
        "user_input_1 = \"Add a ToDo to call to make appointment at Yoga.\"\n",
        "user_input_2 = \"Actually, add a ToDo to drop by Yoga in person on Sunday.\"\n",
        "config = {\"configurable\": {\"user_id\": \"Test-Double-Texting\"}}\n",
        "graph_name = \"task_maistro\"\n",
        "\n",
        "rolled_back_run = await client.runs.create(\n",
        "    thread[\"thread_id\"],\n",
        "    graph_name,\n",
        "    input={\"messages\": [HumanMessage(content=user_input_1)]},\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "second_run = await client.runs.create(\n",
        "    thread[\"thread_id\"],\n",
        "    graph_name,\n",
        "    input={\"messages\": [HumanMessage(content=user_input_2)]},\n",
        "    config=config,\n",
        "    multitask_strategy=\"rollback\",\n",
        ")\n",
        "\n",
        "# Wait until the second run completes\n",
        "await client.runs.join(thread[\"thread_id\"], second_run[\"run_id\"])\n",
        "\n",
        "# Get the state of the thread\n",
        "state = await client.threads.get_state(thread[\"thread_id\"])\n",
        "for m in convert_to_messages(state[\"values\"][\"messages\"]):\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVqQAkX5cFdn"
      },
      "source": [
        "The initial run was deleted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDAfPK9PcFdo",
        "outputId": "eb28539a-1ac6-456c-f8cb-580a01867471"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original run was correctly deleted\n"
          ]
        }
      ],
      "source": [
        "# Confirm that the original run was deleted\n",
        "try:\n",
        "    await client.runs.get(thread[\"thread_id\"], rolled_back_run[\"run_id\"])\n",
        "except httpx.HTTPStatusError as _:\n",
        "    print(\"Original run was correctly deleted\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btJyEQc-cFdo"
      },
      "source": [
        "### Summary\n",
        "\n",
        "We can see [all the methods summarized](https://langchain-ai.github.io/langgraph/concepts/double_texting/):\n",
        "\n",
        "![Screenshot 2024-11-15 at 12.13.18 PM.png](attachment:ff0af98b-71b1-497a-9c0e-b3519662fd2c.png)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}